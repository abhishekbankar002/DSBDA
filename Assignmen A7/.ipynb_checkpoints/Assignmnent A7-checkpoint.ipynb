{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DFfipp9dWyp",
    "outputId": "4efdd478-48a1-4844-c27d-8b9d6eab6cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import docx\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV05UfLuhZNP"
   },
   "outputs": [],
   "source": [
    "word_Doc=docx.Document('sample_data.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tanPAuvdh3Ek"
   },
   "outputs": [],
   "source": [
    "extractedText = word_Doc.paragraphs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2Hz-Ts0h9AP",
    "outputId": "750e722f-fb6c-4588-f8c4-d84b5f7be1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(\"The quick brown fox jumps over the lazy dog\")\n",
    "nltk.download('stopwords')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skJC7gHPiqQs",
    "outputId": "8126f59b-38ae-41ff-b35b-423f2fa6def6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kLNePQGj6eE",
    "outputId": "2522bd1b-60d5-4fd7-c15f-d744f06c6ec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zzAlIMOkPxA",
    "outputId": "d1168b5d-0541-40b6-a8dc-c5593cad4dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stems = []\n",
    "for t in tokens:    \n",
    "    stems.append(porter.stem(t))\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjsnSTqWkgiy",
    "outputId": "98703090-a3a2-408e-fbbc-28589245deaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "jump\n",
      "lazy\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for w in tokens:\n",
    "  print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8pzcb_2lBMZ"
   },
   "outputs": [],
   "source": [
    "first_sentence = word_Doc.paragraphs[1].text\n",
    "second_sentence = word_Doc.paragraphs[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgzYwmFRndp8",
    "outputId": "c0eadbe4-6dd6-4c5d-8db4-f8958e9b3521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job', 'machine', 'key', 'best', 'Data', 'the', 'of', '21st', 'is', 'science', 'for', 'Science', 'century', 'learning', 'data'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JebwBPrnoO05",
    "outputId": "2bab65b4-c0fe-4ee5-9ff0-b91b267beaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job  machine  key  best  Data  the  of  21st  is  science  for  Science  \\\n",
      "0    1        0    0     1     1    2   1     1   1        0    0        1   \n",
      "1    0        1    1     0     0    1   0     0   1        1    1        0   \n",
      "\n",
      "   century  learning  data  \n",
      "0        1         0     0  \n",
      "1        0         1    18  \n"
     ]
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "\n",
    "print(pd.DataFrame([wordDictA, wordDictB]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZ0k5orsoh9J",
    "outputId": "1359b674-d486-4995-ee02-3d149f164b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job  machine   key  best  Data   the   of  21st    is  science   for  \\\n",
      "0  0.1     0.00  0.00   0.1   0.1  0.20  0.1   0.1  0.10     0.00  0.00   \n",
      "1  0.0     0.04  0.04   0.0   0.0  0.04  0.0   0.0  0.04     0.04  0.04   \n",
      "\n",
      "   Science  century  learning  data  \n",
      "0      0.1      0.1      0.00  0.00  \n",
      "1      0.0      0.0      0.04  0.72  \n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzYRG7S4opcw",
    "outputId": "7f7a91b3-7a2c-4b8c-b2c5-69a5e5a35e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job': 0.3010299956639812, 'machine': 0.3010299956639812, 'key': 0.3010299956639812, 'best': 0.3010299956639812, 'Data': 0.3010299956639812, 'the': 0.3010299956639812, 'of': 0.3010299956639812, '21st': 0.3010299956639812, 'is': 0.3010299956639812, 'science': 0.3010299956639812, 'for': 0.3010299956639812, 'Science': 0.3010299956639812, 'century': 0.3010299956639812, 'learning': 0.3010299956639812, 'data': 0.3010299956639812}\n"
     ]
    }
   ],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "print(idfs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
